# -*- coding: utf-8 -*-
"""Siamese-networks.ipynb online_mining ? adlı not defterinin kopyası

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10CFdVvrXsRSoZhiWz-N3rzP4aiCBetEt
"""

# #%%script false --no-raise-error
# #!unzip "orl.zip" -d .
# #!pip install --upgrade matplotlib
# !cp /content/drive/MyDrive/CASIA-WebFace.zip /content/
# !unzip /content/CASIA-WebFace.zip
from torchvision.datasets import LFWPeople, LFWPairs
LFWPeople("/content/", download = True)

# ! cp  /content/drive/MyDrive/colorferet.tar /content/
# !tar -xvf "/content/colorferet.tar" -C "/content/feret"

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader,Dataset
import matplotlib.pyplot as plt
import torchvision.utils
import numpy as np
import random
from PIL import Image
import torch
from torch.autograd import Variable
import PIL.ImageOps    
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

from tqdm import tqdm
import os
import random
import copy

class CasiaWebFaceTriplets(Dataset):
  def __init__(self, path, transform=None):
    super(CasiaWebFaceTriplets, self).__init__()
    self.casiaImages = {}
    self.image_size = 0
    self.triplets = []
    self.transform = transform
    ldir = os.listdir(path)
    random.shuffle(ldir)
    for identity in tqdm(ldir):
      self.casiaImages[identity] = []
      class_path = path + "/" + identity
      for image in os.listdir(class_path):
        self.casiaImages[identity].append(class_path + "/" + image)
        self.image_size += 1
    
    self.generateRandomTriplets()

  def generateRandomTriplets(self, tries = 1):
    self.triplets = []
    t = copy.deepcopy(self.casiaImages)
    for person in self.casiaImages:
      random.shuffle(self.casiaImages[person])
    p1_per_img = 8
    print(f"image size {self.image_size}")
    for tries in range(tries):
      for person1 in self.casiaImages:
        person1Images = self.casiaImages[person1]
        if(len(person1Images) >= 2):
          triplet = (person1Images.pop(0), person1Images.pop(0), 0)
          for person2 in self.casiaImages:
            person2Images = self.casiaImages[person2]
            if (person1 == person2) or (len(person2Images) < 1): continue
            rn = 0
            if(len(person2Images) > 1):
              rn = np.random.randint(0, len(person2Images) - 1) 
            triplet = (triplet[0], triplet[1], person2Images[rn])
            self.triplets.append(triplet)
            
    l = len(self.triplets)
    print(f"generated random triplets {l}")
    random.shuffle(self.triplets)
    self.casiaImages = t 
    
    
  def __len__(self):
        return len(self.triplets)

  def __getItemHelper(self,img0, img1, img2):
    img0 = Image.open(img0).convert("RGB")
    img1 = Image.open(img1).convert("RGB")
    img2 = Image.open(img2).convert("RGB")

    if(self.transform != None):
      img0 = self.transform(img0)
      img1 = self.transform(img1)
      img2 = self.transform(img2)
    
    return img0, img1, img2

  def __getitem__(self,index):


    img0 = Image.open(self.triplets[index][0]).convert("RGB")
    img1 = Image.open(self.triplets[index][1]).convert("RGB")
    img2 = Image.open(self.triplets[index][2]).convert("RGB")

    if(self.transform != None):
      img0 = self.transform(img0)
      img1 = self.transform(img1)
      img2 = self.transform(img2)
    
    return img0, img1, img2
  
  def generateHarderTriplets(self, net, criterion, numOfTriplets = 1000, deleteFormer = True):
    hard = []
    semi_hard = []
    net.eval()
    doneGenerating = False
    pdist = nn.PairwiseDistance(p = 2)
    tindex = 0
    for data in tqdm(self.triplets):
      with torch.no_grad():
        img0, img1 , img2 = self.__getItemHelper(data[0], data[1], data[2])
        img0 = img0[None, :]
        img1 = img1[None, :]
        img2 = img2[None, :]
        img0, img1 , img2 = img0.cuda(), img1.cuda() , img2.cuda()
        output1,output2,output3 = net(img0,img1,img2)
        
        if(pdist(output1, output3) < pdist(output1, output2)):
          hard.append((data[0], data[1], data[2]))
          numOfTriplets -= 1
          tindex += 1
          if(numOfTriplets < 0):
            doneGenerating = True
            break
        if(doneGenerating):
          break
    lt = len(self.triplets)
    print(f"generated hard triplets {tindex}/{lt}")
    if deleteFormer:
      self.triplets = hard
    else:
      self.triplets += hard


def imshow(img,text=None,should_save=False):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

def show_plot(iteration, train_loss, vld_loss):
    plt.plot(iteration, train_loss, label = "train loss", linestyle="--")
    plt.plot(iteration, vld_loss, label = "validation loss", linestyle="-.")
    plt.legend()
    plt.show()


import torchvision.models as models

class SiameseNetwork(nn.Module):
    def __init__(self):
        super(SiameseNetwork, self).__init__()
        self.cnn1 = nn.Sequential(
            nn.ReflectionPad2d(1),
            nn.Conv2d(1, 4, kernel_size=3),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(4),
            
            nn.ReflectionPad2d(1),
            nn.Conv2d(4, 8, kernel_size=3),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(8),


            nn.ReflectionPad2d(1),
            nn.Conv2d(8, 8, kernel_size=3),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(8),


        )

        self.fc1 = nn.Sequential(
            nn.Linear(8*100*100, 500),
            nn.ReLU(inplace=True),

            nn.Linear(500, 500),
            nn.ReLU(inplace=True),

            nn.Linear(500, 5))
        
        vgg16 = models.vgg16(pretrained=True)

        for param in vgg16.features[:24]:
            param.require_grad = False

        num_features = vgg16.classifier[0].in_features

        additional = nn.Sequential(
            nn.Linear(num_features, 512),
            nn.ReLU(inplace=True),
            
            nn.Linear(512, 512),
            nn.ReLU(inplace=True),
            
            nn.Linear(512,512),
            nn.ReLU(inplace=True)
        )
        vgg16.classifier = nn.Sequential(*additional) 


        self.vgg16 = vgg16

        

    def forward_once(self, x):
        #output = self.cnn1(x)
        #output = output.view(output.size()[0], -1)
        #output = self.fc1(output)
        output = self.vgg16(x)
        return output

    def forward(self, anchor, neg, pos):
        anchor = self.forward_once(anchor)
        neg = self.forward_once(neg)
        pos = self.forward_once(pos)
        return anchor, neg, pos

sn = SiameseNetwork()


net = torch.load("/content/drive/MyDrive/lfw_margin25_only_vgg_params64_512-512-512-vgg-16-last-block_mining").cuda()
criterion = nn.TripletMarginLoss(margin = 1)

optimizer = optim.Adam(net.parameters(),lr = 0.00001)

from torchvision.datasets import LFWPeople, LFWPairs
transformation = transforms.Compose([transforms.Resize((244,244)),
                                     transforms.ToTensor()
                                    ])

tripletTrain = CasiaWebFaceTriplets("/content/lfw-py/lfw_funneled", transform=transforms.Compose([transforms.Resize((244,244)),
                                                                      transforms.ToTensor()
                                                                      ]))

def splitAndGenerateLoaders(triplets):
  train_size = int(0.8 * len(triplets))
  test_size = len(tripletTrain) - train_size
  train_dataset, test_dataset = torch.utils.data.random_split(tripletTrain, 
                                                              [train_size, test_size])

  test_d = DataLoader(test_dataset,
                          shuffle=True,
                          num_workers=8,
                          batch_size=32)


  train_d = DataLoader(train_dataset,
                          shuffle=True,
                          num_workers=8,
                          batch_size=32)
  print(len(test_d), len(train_d))
  return(test_d, train_d)

import itertools
counter = []
loss_history = [] 
vld_loss_history = [] 
iteration_number= 0

def validateOneBatch(model, data_loader_it):
  model.eval()
  loss = 0
  # for i, data in enumerate(data_loader,0):
  with torch.no_grad():
    data = next(data_loader_it, None)
    if data == None: return None
    img0, img1, img2 = data
    img0, img1, img2 = img0.cuda(), img1.cuda() , img2.cuda()
    output1,output2,output3 = net(img0,img1,img2)
    loss += criterion(output1,output2,output3).item() #with mean reduction
  model.train()
  return loss

    

# print(validateOneBatch(net, it))
# print(validateOneBatch(net, it))
# len(test_dataloader)

tripletTrain.generateRandomTriplets(tries = 1)
tripletTrain.generateHarderTriplets(net, criterion, numOfTriplets = 20)
print(len(tripletTrain))

i = 0
concatenated = torch.cat((tripletTrain[i+0][0].cpu(),tripletTrain[i+0][1].cpu(),tripletTrain[i+0][2].cpu()),2)
imshow(torchvision.utils.make_grid(concatenated))
i+=1
concatenated = torch.cat((tripletTrain[i+0][0].cpu(),tripletTrain[i+0][1].cpu(),tripletTrain[i+0][2].cpu()),2)
imshow(torchvision.utils.make_grid(concatenated))
i+=1
concatenated = torch.cat((tripletTrain[i+0][0].cpu(),tripletTrain[i+0][1].cpu(),tripletTrain[i+0][2].cpu()),2)
imshow(torchvision.utils.make_grid(concatenated))
i+=1
concatenated = torch.cat((tripletTrain[i+0][0].cpu(),tripletTrain[i+0][1].cpu(),tripletTrain[i+0][2].cpu()),2)
imshow(torchvision.utils.make_grid(concatenated))
i+=1
concatenated = torch.cat((tripletTrain[i+0][0].cpu(),tripletTrain[i+0][1].cpu(),tripletTrain[i+0][2].cpu()),2)
imshow(torchvision.utils.make_grid(concatenated))
i+=1
concatenated = torch.cat((tripletTrain[i+0][0].cpu(),tripletTrain[i+0][1].cpu(),tripletTrain[i+0][2].cpu()),2)
imshow(torchvision.utils.make_grid(concatenated))

for epoch in range(0,60):
    if(epoch%3 == 0):
      tripletTrain.generateRandomTriplets(tries = 1)
      tripletTrain.generateHarderTriplets(net, criterion, numOfTriplets = 3000)
      test_dataloader, train_dataloader = splitAndGenerateLoaders(tripletTrain)

    test_loader_it = iter(test_dataloader)
    if(epoch%3 == 0):
      torch.save(net, "/content/drive/MyDrive/lfw_only_vgg_params" + str(epoch) +"_512-512-512-vgg-16-last-block_mining")
    for i, data in enumerate(train_dataloader,0):
        img0, img1 , img2 = data
        img0, img1 , img2 = img0.cuda(), img1.cuda() , img2.cuda()
        optimizer.zero_grad()
        output1,output2,output3 = net(img0,img1,img2)
        loss_contrastive = criterion(output1,output2,output3)
        loss_contrastive.backward()
        optimizer.step()
        if i %4 == 0 :
            if(i%len(test_dataloader) == 0): test_loader_it = iter(test_dataloader)
            vld_loss = validateOneBatch(net, test_loader_it)
            print("Epoch number {}\n Current loss {} Test Set Loss {}\n".format(epoch,loss_contrastive.item(), vld_loss))
            #pda, nda = TestWithLfw()
            #print(f"testing with lfw a-p dist {pda} p-n dist {nda} ratio {pda/nda}")
            iteration_number +=10
            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())
            vld_loss_history.append(vld_loss)
            print(f"")
show_plot(counter,loss_history, vld_loss_history)

r

show_plot(counter,loss_history, vld_loss_history)
torch.save(net, "/content/drive/MyDrive/lfw_pre_more_triplet" + str(13) +"_512-512-512-vgg-16-last-block_mining")

#torch.save(net, "/content/drive/MyDrive/casia_margin_4_0.000001_?_6_epoch")



#dataiter = iter(test_dataloader)


example_batch = next(dataiter)
print(len(example_batch[0]))
concatenated = torch.cat((example_batch[0][:8],example_batch[1][:8],example_batch[2][:8]),0)
imshow(torchvision.utils.make_grid(concatenated))
output1,output2,output3 = net(example_batch[0][3].cuda(),example_batch[1][3].cuda(),example_batch[2][3].cuda())
euclidean_distance = F.pairwise_distance(output1, output2)

def loadImgs(dir, files):
  result = []
  for file in files:
    ff = dir + file
    print(ff)
    img = Image.open(ff).convert("RGB")
    img = transformation(img)
    img = img[None, :]
    result.append(img)
  return result

test_files = ["Abdullah_Gul_0013.jpg",
              "Abdullah_Gul_0014.jpg",
              "Abdullah_Gul_0015.jpg",
              "Abdullah_Gul_0016.jpg",
              "Ahmed_Chalabi_0001.jpg",
              "Ahmed_Chalabi_0002.jpg",
              "Ahmed_Chalabi_0003.jpg"]

imgs = loadImgs("/content/", test_files)

x0 = imgs[0]
x1 = imgs[1]
x2 = imgs[2]

print(x0.shape, x1.shape)
concatenated = torch.cat((x0, x1), 0)
with torch.no_grad():
  output1, output2, output3 = net(x0.cuda(), x1.cuda(), x2.cuda())
  
  euclidean_distance = F.pairwise_distance(output1, output2)
imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')

net = torch.load("/content/drive/MyDrive/casia_norelu")
transformation = transforms.Compose([transforms.Resize((244,244)),
                                     transforms.ToTensor()
                                    ])

orlDataset = CasiaWebFaceTriplets("/content/lfw-py/lfw_funneled")

transformation = transforms.Compose([transforms.Resize((244,244)),
                                     transforms.ToTensor()
                                    ])

# orlDataset = CasiaWebFaceTriplets("/content/orl")
orlDataset = CasiaWebFaceTriplets("/content/lfw-py/lfw_funneled")
false_negative = 0
false_positive = 0
true_negative = 0
true_positive = 0
i = 1000
for a_f, p_f, n_f in orlDataset:
  i -= 1
  if i < 0: break
  a = transformation(a_f)[None, :]
  p = transformation(p_f)[None, :]
  n = transformation(n_f)[None, :]
  with torch.no_grad():
    output1, output2, output3 = net(a.cuda(), p.cuda(), n.cuda())
    positive_dist = F.pairwise_distance(output1, output2) # between anchor and positive 
    negative_dist = F.pairwise_distance(output2, output3) # negative and positive
  p_score = positive_dist.item()
  n_score = negative_dist.item()
  
  treshold = 17

  if(p_score < treshold):
    true_positive += 1
  else:
    false_negative += 1
    #imshow(torchvision.utils.make_grid(torch.cat((a, p), 0)))
    # print(n_score)
  if(n_score > treshold):
    true_negative += 1
  else:
    false_positive += 1
    #imshow(torchvision.utils.make_grid(torch.cat((p, n), 0)))
    # print(n_score)
print(f"true_positive {true_positive} true_negative {true_negative} false_negative {false_negative} false_positive {false_positive}")
total = true_positive + true_negative + false_positive  + false_negative
print(total)
print((true_positive + true_negative) / total)



total = true_positive + true_negative + false_positive  + false_negative
print(total)
print((true_positive + true_negative) / total)

!rm -rf /content/lfw-py/lfw_funneled/*.txt
!rm -rf /content/lfw-py/lfw_funneled/*.tgz
!cp /
